
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_02_compute_sesame_meg_auditory_volume_mne.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_02_compute_sesame_meg_auditory_volume_mne.py>`
        to download the full example code. or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_02_compute_sesame_meg_auditory_volume_mne.py:


=====================================================================
Compute SESAME inverse solution on evoked data in volume source space
=====================================================================

In this example we shall compute SESAME inverse solution on evoked data in
a volume source space. Data are taken from the MNE-Python
`sample <https://mne.tools/stable/generated/mne.datasets.sample.data_path.html#mne.datasets.sample.data_path>`_
dataset and correspond to the response to an auditory stimulus.

.. GENERATED FROM PYTHON SOURCE LINES 11-42

.. code-block:: Python

    # Authors: Gianvittorio Luria <luria@dima.unige.it>
    #          Sara Sommariva <sommariva@dima.unige.it>
    #          Alberto Sorrentino <sorrentino@dima.unige.it>
    #
    # License: BSD (3-clause)

    # sphinx_gallery_thumbnail_number = 3

    from os import path as op
    import matplotlib.pyplot as plt
    import numpy as np
    from nilearn.plotting import plot_stat_map
    from nilearn.image import index_img
    from mne import read_forward_solution, pick_types_forward, read_evokeds, \
        read_trans, head_to_mri
    from mne.datasets import sample
    from mne.label import _n_colors
    from sesameeg.mne import prepare_sesame
    import time


    data_path = sample.data_path()
    subject = 'sample'
    subjects_dir = op.join(data_path, 'subjects')
    fname_fwd = op.join(data_path, 'MEG', subject,
                        'sample_audvis-meg-vol-7-fwd.fif')
    fname_trans = op.join(data_path, 'MEG', subject,
                          'sample_audvis_raw-trans.fif')
    fname_t1 = op.join(data_path , 'subjects', subject, 'mri', 'T1.mgz')
    fname_evoked = op.join(data_path, 'MEG', subject, 'sample_audvis-ave.fif')








.. GENERATED FROM PYTHON SOURCE LINES 43-47

Load the  mri-to-head coordinates transformation matrix, the forward solution
:math:`\textbf{G}` and the evoked data :math:`\textbf{y}`.
The forward solution also defines the employed brain discretization which, in this example,
comprises the whole brain volume.

.. GENERATED FROM PYTHON SOURCE LINES 47-64

.. code-block:: Python


    # Transformation matrix
    trans = read_trans(fname_trans)

    # Choose sensor type
    meg_sensor_type = True  # All meg sensors will be included
    eeg_sensor_type = False

    # Forward solution
    fwd = read_forward_solution(fname_fwd, exclude='bads')
    fwd = pick_types_forward(fwd, meg=meg_sensor_type, eeg=eeg_sensor_type, ref_meg=False)

    # Evoked Data
    condition = 'Left Auditory'
    evoked = read_evokeds(fname_evoked, condition=condition, baseline=(None, 0))
    evoked = evoked.pick('meg', exclude='bads')





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Reading forward solution from /home/pasca/mne_data/MNE-sample-data/MEG/sample/sample_audvis-meg-vol-7-fwd.fif...
        Reading a source space...
        [done]
        1 source spaces read
        Desired named matrix (kind = 3523 (FIFF_MNE_FORWARD_SOLUTION_GRAD)) not available
        Read MEG forward solution (3757 sources, 306 channels, free orientations)
        Source spaces transformed to the forward solution coordinate frame
        305 out of 306 channels remain after picking
        305 out of 305 channels remain after picking
    Reading /home/pasca/mne_data/MNE-sample-data/MEG/sample/sample_audvis-ave.fif ...
        Read a total of 4 projection items:
            PCA-v1 (1 x 102) active
            PCA-v2 (1 x 102) active
            PCA-v3 (1 x 102) active
            Average EEG reference (1 x 60) active
        Found the data of interest:
            t =    -199.80 ...     499.49 ms (Left Auditory)
            0 CTF compensation matrices available
            nave = 55 - aspect type = 100
    Projections have already been applied. Setting proj attribute to True.
    Applying baseline correction (mode: mean)




.. GENERATED FROM PYTHON SOURCE LINES 65-66

Define the parameters.

.. GENERATED FROM PYTHON SOURCE LINES 66-88

.. code-block:: Python

    time_min, time_max = 0.045, 0.135  # Select N100m
    subsample = None
    sample_min, sample_max = evoked.time_as_index([time_min, time_max], use_rounding=True)

    # To accelerate the run time of this example, we use a small number of
    # particles. We recall that the parameter ``n_parts`` represents, roughly speaking,
    # the number of candidate solutions that are tested in the Monte Carlo procedure;
    # larger values yield in principle more accurate reconstructions but also entail a
    # higher computational cost. Setting the value to about a hundred seems to represent
    # a good tradeâ€“off.
    n_parts = 10
    # If None, noise_std and dip_mom_std will be estimated by SESAME.
    noise_std = None
    dip_mom_std = None
    noise_cov = None

    # You can make SESAME pre-whiten the data by providing a noise covariance
    # from mne import read_cov
    # fname_cov = op.join(sample.data_path(), 'MEG', subject,
    #                    'sample_audvis-cov.fif')
    # noise_cov = read_cov(fname_cov)








.. GENERATED FROM PYTHON SOURCE LINES 89-90

Visualize the selected data.

.. GENERATED FROM PYTHON SOURCE LINES 90-98

.. code-block:: Python


    lst = evoked.plot_joint(show=False)
    for fig in lst:
        ax = fig.get_axes()
        ax[0].axvline(time_min, color='r', linewidth=2.0)
        ax[0].axvline(time_max, color='r', linewidth=2.0)
    plt.show()




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_plot_02_compute_sesame_meg_auditory_volume_mne_001.png
         :alt: Gradiometers (203 channels), 0.090 s, 0.162 s, 0.251 s
         :srcset: /auto_examples/images/sphx_glr_plot_02_compute_sesame_meg_auditory_volume_mne_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_plot_02_compute_sesame_meg_auditory_volume_mne_002.png
         :alt: Magnetometers (102 channels), 0.090 s, 0.166 s, 0.246 s
         :srcset: /auto_examples/images/sphx_glr_plot_02_compute_sesame_meg_auditory_volume_mne_002.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Projections have already been applied. Setting proj attribute to True.




.. GENERATED FROM PYTHON SOURCE LINES 99-100

Apply SESAME.

.. GENERATED FROM PYTHON SOURCE LINES 100-113

.. code-block:: Python

    _sesame = prepare_sesame(fwd, evoked, n_parts=n_parts, noise_std=noise_std,
                             top_min=time_min, top_max=time_max, dip_mom_std=dip_mom_std,
                             hyper_q=True, noise_cov=noise_cov, subsample=subsample,
                             subject=subject, subjects_dir=subjects_dir, trans_matrix=trans)
    time_start = time.time()
    _sesame.apply_sesame()
    time_elapsed = (time.time() - time_start)
    print(f'    Total computation time: {round(time_elapsed, 2)} seconds.')

    # Compute goodness of fit
    gof = _sesame.goodness_of_fit()
    print('    Goodness of fit with the recorded data: {0}%'.format(round(gof, 4) * 100))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Computing inverse operator with 305 channels.
        305 out of 305 channels remain after picking
    Forward model with free source orientation.
    Computing neighbours matrix [done]
    Computing neighbours probabilities...[done]
    Analyzing data from 0.045 s to 0.1349 s
    Estimating dipole moment std...[done]
     Estimated dipole moment std: 3.5816e-08
    Sampling hyperprior for dipole moment std.
    Estimating noise std...[done]
     Estimated noise std: 3.9983e-12
    Computing inverse solution. This will take a while...
    Estimated dipole strength variance: 2.6413825716435953e-08
        Estimated number of sources: 2
        Estimated source locations:
            * source 1: [-0.06009     0.00132211  0.0562192 ]
            * source 2: [0.05158217 0.00683185 0.06580936]
    [done in 97 iterations]
        Total computation time: 12.31 seconds.
        Goodness of fit with the recorded data: 68.69%




.. GENERATED FROM PYTHON SOURCE LINES 114-116

Visualize the posterior map of the dipoles' location
:math:`p(r| \textbf{y}, 2)` as an overlay onto the MRI.

.. GENERATED FROM PYTHON SOURCE LINES 116-118

.. code-block:: Python

    _sesame.plot_sources(plot_kwargs={'distance': 650})




.. image-sg:: /auto_examples/images/sphx_glr_plot_02_compute_sesame_meg_auditory_volume_mne_003.png
   :alt: plot 02 compute sesame meg auditory volume mne
   :srcset: /auto_examples/images/sphx_glr_plot_02_compute_sesame_meg_auditory_volume_mne_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Volume stc computed  
    WARNING! Removing invalid plot_stat_map keyword : distance




.. GENERATED FROM PYTHON SOURCE LINES 119-120

Visualize amplitude of the estimated sources as function of time.

.. GENERATED FROM PYTHON SOURCE LINES 120-122

.. code-block:: Python

    _sesame.plot_source_amplitudes()




.. image-sg:: /auto_examples/images/sphx_glr_plot_02_compute_sesame_meg_auditory_volume_mne_004.png
   :alt: plot 02 compute sesame meg auditory volume mne
   :srcset: /auto_examples/images/sphx_glr_plot_02_compute_sesame_meg_auditory_volume_mne_004.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 123-124

Save results.

.. GENERATED FROM PYTHON SOURCE LINES 124-130

.. code-block:: Python


    # You can save SESAME result in an HDF5 file with:
    # _sesame.save_h5(save_fname, sbj=subject, data_path=fname_evoked, fwd_path=fname_fwd)

    # You can save SESAME result in a Pickle file with:
    # _sesame.save_pkl(save_fname, sbj=subject, data_path=fname_evoked, fwd_path=fname_fwd)








.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 19.629 seconds)


.. _sphx_glr_download_auto_examples_plot_02_compute_sesame_meg_auditory_volume_mne.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/pybees/sesameeg/master?urlpath=lab/tree/notebooks/auto_examples/plot_02_compute_sesame_meg_auditory_volume_mne.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_02_compute_sesame_meg_auditory_volume_mne.ipynb <plot_02_compute_sesame_meg_auditory_volume_mne.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_02_compute_sesame_meg_auditory_volume_mne.py <plot_02_compute_sesame_meg_auditory_volume_mne.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_02_compute_sesame_meg_auditory_volume_mne.zip <plot_02_compute_sesame_meg_auditory_volume_mne.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
